{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度神经网络中,　超参数的调整是一项必备技能,　通过观察在训练过程中的检测指标如 loss 和准确率来判断当前模型处于什么训练状态.　及时调整超参数以更科学的训练模型能够提高资源的利用率. 本文研究使用了以下超参数, 下面分别介绍并总结了不同超参数的调整规则."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率(learning_rate) 是指优化算法中更新网络权重的幅度大小, 学习率可以是恒定的、逐渐降低的、基于动量的或者适应的. 不同优化算法决定不同的学习率,　当学习率过大则可能导致模型不收敛,　损失 loss 不断上下震荡,　学习率过小模型收敛速度偏慢,　需要更长的时间训练. 通常 learning_rate 取值为 [0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批次大小的 batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批次大小是每一次训练神经网络送入模型的样本数,　在卷积神经网络中, 大批次通常可使网络更快收敛,　但由于内存资源的限制, 批次过大可能导致内存不够用或程序内核崩溃. batch size 通常取值为. [16, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器 optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前 Adam　是最快速收敛且场被使用的优化器,　随机梯度(SGD)　虽然收敛偏慢,　但是加入动量 Momentum 可加快收敛,　同时待动量的随机梯度下降算法是更好的最优解, 即模型收敛后会有更高的准确性.　通常苦追求速度则用 Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 迭代次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代次数是指整个训练集输入到神经网络训练的次数,　当测试错误率和训练错误率相差较小时,　可认为当前迭代次数合适,　当测试错误率先变小后变大时则说明迭代次数过大了, 需要减小迭代次数, 否则容易出现过拟合."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在神经网络中激活函数不是真的激活什么,　而是用激活函数给神经网络加入一些非线性因素, 使得网络可以更好地解决较为复杂的问题.　比如有些问题是线性可分的, 而现实场景张更多的问题不是线性可分的, 若不用激活函数则难以拟合非线性问题, 测试会有低准确率, 所以激活函数主要是非线性的, sigmoid tamh relu. sigmoid 通常用于二分类,　但要防止梯度消失,　故适合浅层神经网络且需要配备较小的初始化权重,　tanh　具有中心对称,　适用与对称性的二分类. relu　是使用最多的激活函数, 简单又避免了梯度消失."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 数据增强,　由于需要的data较多又得不到满足，则可以通过对颜色（color）、尺度（scale）、裁剪（crop）、翻转（Flip）、添加噪声（Noise）、旋转（Rotation）等等，这样就增加了数据集的数目，解决的是data不足的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Pre-processing（数据预处理）那要对数据进行预处理，预处理的前提是要正确的理解数据。你用了两张一模一样的数据在训练集里，意义何在？所以接下来要讲的数据预处理很重要。常用的方法有：normalization（归一化）、PCA（主成分分析）、Whitening（白化）等。\n",
    "\n",
    "1. Normalization。可以这样认为，归一化主要在干这样一件事：把数据从一个一般的分布，变成0均值、单位方差的分布，为什么这么干呢？原因是这么做更容易收敛。Batch Normalization（BN）是一个升级版本，作者主要考虑当使用了饱和的激活函数时，例如sigmoid函数变成0均值、单位方差则会出现在该函数中间近似线性的那一段，这是非常糟糕的，中间线性的部分的拟合能力最差，因此降低了模型的表达capacity，所以BN应运而生，实验表明，其效果sigmoid函数比Relu函数要好。\n",
    "\n",
    "1. PCA。研究如何以最少的信息丢失将众多原有的变量信息浓缩到少数几个维度上，即所谓的主成分。首先计算出协方差矩阵，然后求出协方差矩阵的特征向量，并用其对原特征进行线性变换，实现降维。\n",
    "\n",
    "1. Whitening。去除特征向量中各个特征之间的相关性，同时保证每个特征的方差一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重的初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化项\n",
    "\n",
    "Regularizations（正则化)正则化应该讲是一种避免over-fitting的有效方法，\n",
    "\n",
    "**增加数据集和减小模型的复杂度** 此处正则化就是从减小模型的复杂度出发的一项技术，其本质就是控制模型学习的特征数目，使其最小化，从而防止在训练过程中引入训练集的抽样误差，正则化包括L2正则化和L1正则化。\n",
    "\n",
    "L1和L2是正则化项，又叫做罚项，是为了限制模型的参数，防止模型过拟合而加在损失函数后面的一项。\n",
    "\n",
    "1. L1是模型各个参数的绝对值之和。\n",
    "\n",
    "   L2是模型各个参数的平方和的开方值。\n",
    "\n",
    "2. L1会趋向于产生少量的特征，而其他的特征都是0.\n",
    "\n",
    "因为最优的\n",
    "参数值很大概率出现在坐标轴上，这样就会导致某一维的权重为0 ，产生稀疏权重矩阵\n",
    "\n",
    "L2会选择更多的特征，这些特征都会接近于0。  \n",
    "\n",
    "最优的参数值很小概率出现在坐标轴上，因此每一维的参数都不会是0。当最小化||w||时，就会使每一项趋近于0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout。Dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃，如下图所示。对于随机梯度下降来说，由于是随机丢弃，因此每一个mini-batch都在训练不同的网络（对于一个有N个节点的神经网络，采用dropout后，可以认为其是2n个模型的集合），同时每个网络只见过一个训练数据（每次都是随机的新网络），从而将这些多个模型组合起来，以每个模型的平均输出作为结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
